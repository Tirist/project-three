# Project Three - Stock Evaluation Pipeline

A comprehensive, production-ready data pipeline for stock market analysis with automated orchestration, integrity monitoring, and frontend dashboard preparation.

## ğŸ—ï¸ Project Structure

```
project_root/
â”œâ”€â”€ pipeline/                    # Core pipeline components
â”‚   â”œâ”€â”€ fetch_tickers.py        # S&P 500 ticker fetching
â”‚   â”œâ”€â”€ fetch_data.py           # Historical price data collection
â”‚   â”œâ”€â”€ process_features.py     # Feature engineering and processing
â”‚   â”œâ”€â”€ run_pipeline.py         # Main pipeline orchestrator
â”‚   â””â”€â”€ utils/                  # Shared utilities
â”‚       â”œâ”€â”€ common.py           # Common functions and classes
â”‚       â”œâ”€â”€ logger.py           # Standardized logging
â”‚       â”œâ”€â”€ progress.py         # Progress tracking utilities
â”‚       â””â”€â”€ integrity_monitor.py # Pipeline monitoring
â”œâ”€â”€ reports/                    # Reporting and API
â”‚   â”œâ”€â”€ generate_integrity_report.py
â”‚   â”œâ”€â”€ api.py                  # Frontend API endpoints
â”‚   â””â”€â”€ integrity_reports/      # Generated reports
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ test_fetch_data.py
â”‚   â”œâ”€â”€ test_fetch_tickers.py
â”‚   â”œâ”€â”€ test_process_features.py
â”‚   â””â”€â”€ run_all_tests.py
â”œâ”€â”€ scripts/                    # Automation scripts
â”‚   â”œâ”€â”€ cleanup_old_reports.py  # Data retention management
â”‚   â””â”€â”€ setup_cron.sh          # Cron job setup
â”œâ”€â”€ tools/                      # Maintenance and utilities
â”‚   â””â”€â”€ maintenance/           # Pipeline maintenance tools
â”‚       â””â”€â”€ fixes/             # Recovery and fix scripts
â”‚           â”œâ”€â”€ populate_historical.py  # Historical data recovery
â”‚           â””â”€â”€ README.md       # Fix documentation
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ raw/                    # Raw CSV data (partitioned by date)
â”‚   â”œâ”€â”€ processed/              # Processed parquet files
â”‚   â””â”€â”€ test/                   # Test data (isolated)
â”œâ”€â”€ logs/                       # Logging and metadata
â”‚   â”œâ”€â”€ fetch/                  # Data fetching logs
â”‚   â”œâ”€â”€ features/               # Feature processing logs
â”‚   â”œâ”€â”€ tickers/                # Ticker fetching logs
â”‚   â”œâ”€â”€ integrity/              # Integrity monitoring logs
â”‚   â””â”€â”€ structured/             # JSON logs for frontend
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ settings.yaml           # Main pipeline settings
â”‚   â””â”€â”€ test_schedules.yaml     # Test scheduling configuration
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ pytest.ini                 # Test configuration
â””â”€â”€ .gitignore                 # Git ignore rules
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Virtual environment (recommended)
- Access to financial data APIs (Alpha Vantage, Yahoo Finance)

### Installation
```bash
# Clone the repository
git clone <repository-url>
cd project-three

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up automated scheduling
bash scripts/setup_cron.sh
```

### Manual Pipeline Run
```bash
# Full production run (all 503 S&P 500 tickers)
python pipeline/run_pipeline.py --prod

# Test run (5 tickers, isolated test data)
python pipeline/run_pipeline.py --test

# Daily integrity test
python pipeline/run_pipeline.py --daily-integrity

# Weekly integrity test
python pipeline/run_pipeline.py --weekly-integrity
```

## ğŸ“Š Automated Schedule

The pipeline runs automatically with the following schedule:

- **5:30 PM ET Daily**: Full production run (503 tickers) - Uses `--daily-integrity` for production mode
  - Runs after market close (4:00 PM ET) to capture complete day's trading data
  - Fresh production data ready by 7:30 PM ET
- **2:00 AM Daily**: Test data cleanup (`--test-only` preserves production data)
- **3:00 AM Sundays**: Production data cleanup (30-day retention)
- **Every 15 minutes**: Integrity monitoring

### Cron Job Verification
```bash
# Check current cron jobs
crontab -l

# Verify cron configuration
python pipeline/utils/integrity_monitor.py --check-cron

# Test cron setup
bash scripts/test_cron_setup.sh
```

## ğŸ§ª Testing

### Current Test Status
- âœ… **21/21 tests passing** (8m 40s runtime)
- âœ… **Comprehensive validation** of data quality and pipeline integrity
- âœ… **Column format support** for both standard and Alpha Vantage data formats
- âœ… **Metadata validation** with complete tracking fields
- âœ… **Error handling** and recovery mechanisms

### Run All Tests
```bash
python tests/run_all_tests.py
```

### Run Specific Test Categories
```bash
# Quick tests only
pytest tests/ -m quick

# Full test suite
pytest tests/ -m heavy

# Specific test file
pytest tests/test_fetch_data.py
```

### Test Coverage
```bash
pytest tests/ --cov=pipeline --cov-report=html
```

## ğŸ”§ Configuration

### Pipeline Settings (`config/settings.yaml`)
```yaml
# Data sources
data_sources:
  primary: "alpha_vantage"
  secondary: "yfinance"
  
# API configuration
api:
  alpha_vantage_key: "your_key_here"
  rate_limit_delay: 12  # seconds between requests
  
# Processing settings
processing:
  parallel_workers: 4
  batch_size: 50
  retention_days: 30
```

### Test Schedules (`config/test_schedules.yaml`)
```yaml
daily_tests:
  enabled: true
  timeout_minutes: 30
  parallel_workers: 2
  
weekly_tests:
  enabled: true
  timeout_minutes: 120
  parallel_workers: 8
  
notifications:
  enabled: false
  webhook_url: "https://hooks.slack.com/..."
```

## ğŸ“ˆ Monitoring & Reporting

### Integrity Reports
```bash
# Generate latest report
python reports/generate_integrity_report.py --type daily

# Generate weekly report
python reports/generate_integrity_report.py --type weekly --date 2025-07-27
```

### Pipeline Status
```bash
# Check pipeline status
python pipeline/utils/integrity_monitor.py --monitor-pipeline

# Verify cron configuration
python pipeline/utils/integrity_monitor.py --check-cron

# Retry failed runs
python pipeline/utils/integrity_monitor.py --retry-failed
```

## ğŸŒ Frontend API

### Start API Server
```bash
python reports/api.py --port 8080
```

### Available Endpoints
- `GET /api/status` - Overall pipeline status
- `GET /api/reports/latest` - Latest integrity report
- `GET /api/reports/daily` - Daily reports
- `GET /api/reports/weekly` - Weekly reports
- `GET /api/data/freshness` - Data freshness information
- `GET /api/pipeline/runs` - Recent pipeline runs

### Example API Response
```json
{
  "timestamp": "2025-07-27T14:30:00",
  "status": "success",
  "data": {
    "pipeline_status": {
      "running": false,
      "last_run": "2025-07-27T04:00:00",
      "next_scheduled": "4:00 AM daily (production)"
    },
    "data_status": {
      "raw_data": {
        "latest_partition": "2025-07-27",
        "partition_count": 7,
        "partitions": ["2025-07-21", "2025-07-25", "2025-07-27"]
      }
    }
  }
}
```

## ğŸ” Data Structure

### Raw Data (`data/raw/dt=YYYY-MM-DD/`)
- CSV files for each S&P 500 ticker
- **Column formats supported**:
  - Standard: `date, open, high, low, close, volume`
  - Alpha Vantage: `date, 1. open, 2. high, 3. low, 4. close, 5. volume`
- Partitioned by date for efficient querying

### Processed Data (`data/processed/dt=YYYY-MM-DD/`)
- `features.parquet` - Consolidated feature dataset
- Technical indicators and derived features
- Optimized for analysis and modeling

### Logs (`logs/`)
- Structured JSON logs for frontend consumption
- **Enhanced metadata** with comprehensive tracking:
  - `run_date`, `tickers_successful`, `features_generated`
  - `runtime_seconds`, `status`, `error_message`
  - `skipped_tickers`, `rows_dropped_due_to_nans`
- Performance metrics and error tracking

## ğŸ› ï¸ Development

### Adding New Features
1. Create feature in appropriate pipeline module
2. Add tests in `tests/` directory
3. Update configuration if needed
4. Update documentation

### Code Style
- Follow PEP 8 guidelines
- Use type hints
- Add docstrings for all functions
- Run tests before committing

### Common Utilities
The `pipeline/utils/` module provides:
- `PipelineConfig`: Centralized configuration management
- `DataManager`: Data directory and file operations
- `LogManager`: Standardized logging and metadata
- `ProgressTracker`: Progress bar utilities
- `IntegrityMonitor`: Pipeline monitoring and reporting

## ğŸ“‹ Maintenance

### Data Cleanup
```bash
# Clean test data only
python scripts/cleanup_old_reports.py --test-only

# Clean production data (30-day retention)
python scripts/cleanup_old_reports.py --pipeline-data --retention-days=30

# Clean everything
python scripts/cleanup_old_reports.py --all
```

### Log Management
```bash
# Manual log rotation
bash scripts/rotate_logs.sh

# View recent logs
tail -f logs/cron_daily.log
```

### Troubleshooting
1. Check cron jobs: `crontab -l`
2. Verify Python environment: `which python`
3. Test pipeline manually: `python pipeline/run_pipeline.py --test`
4. Check integrity: `python pipeline/utils/integrity_monitor.py --check-cron`
5. Run test suite: `python tests/run_all_tests.py`

### Recovery and Maintenance
```bash
# Recover missing historical data
python tools/maintenance/fixes/populate_historical.py

# Check available maintenance scripts
ls tools/maintenance/fixes/

# View maintenance documentation
cat tools/maintenance/fixes/README.md
```

## ğŸ¯ Recent Improvements

### Pipeline Bug Fixes (July 2025)
- âœ… **Fixed missing `--skip-process` argument** in run_pipeline.py argument parser
- âœ… **Improved NaN handling** in process_features.py to preserve data with technical indicators
- âœ… **Added missing dependencies** (fastparquet, tqdm) to requirements.txt
- âœ… **Created maintenance scripts** for historical data recovery and pipeline fixes
- âœ… **Enhanced data validation** and error recovery mechanisms

### Project Consolidation (July 2025)
- âœ… **Reorganized project structure** for better maintainability
- âœ… **Enhanced testing suite** with 21/21 tests passing
- âœ… **Improved metadata generation** with comprehensive tracking
- âœ… **Fixed column format validation** for multiple data sources
- âœ… **Configured production cron jobs** for automated daily runs
- âœ… **Enhanced error handling** and recovery mechanisms

### Key Features
- **Multi-format data support**: Handles both standard and Alpha Vantage CSV formats
- **Comprehensive metadata**: Complete tracking of pipeline execution and data quality
- **Production automation**: Fully automated daily pipeline runs with monitoring
- **Robust testing**: Comprehensive test suite with data validation
- **Integrity monitoring**: Continuous pipeline health monitoring

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make changes with tests
4. Run full test suite: `python tests/run_all_tests.py`
5. Submit pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For issues and questions:
1. Check the troubleshooting section
2. Review logs in `logs/` directory
3. Run integrity checks: `python pipeline/utils/integrity_monitor.py --check-cron`
4. Run test suite: `python tests/run_all_tests.py`
5. Create an issue with detailed information

---

**Last Updated**: July 27, 2025  
**Version**: 1.0.0  
**Status**: âœ… Production Ready - All tests passing, cron jobs configured  
**Test Status**: 21/21 tests passing (8m 40s runtime)  
**Cron Status**: Configured for daily 4:00 AM production runs 